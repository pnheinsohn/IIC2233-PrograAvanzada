{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<font size='5' face='Georgia, Arial'>IIC-2233 Apunte Programación Avanzada</font><br>\n",
    "<font size='1'>&copy; 2015 Karim Pichara - Christian Pieringer. Todos los derechos reservados.</font>\n",
    "<br>\n",
    "<font size='1'> Modificado en 2017-2 por Equipo Docente IIC2233</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing \n",
    "\n",
    "Muchos programadores concuerdan con que \"testing\" es uno de los aspectos más importantes en el desarrollo de software. \n",
    "Testing es el arte de generar código capaz de poner a prueba nuestros programas, chequeando que nuestro desarrollo pasa todas\n",
    "las pruebas en las cuales será sometido por los usuarios finales. Si bien nosotros en general realizamos pruebas manuales cada\n",
    "vez que desarrollamos un nuevo pedazo de código que realiza alguna tarea, es muy probable que nuestro testeo manual haya sido\n",
    "probado con un caso bastante común, lo cual no asegura que nuestro nuevo pedazo de código funciona en todos los posibles casos.\n",
    "Otro factor importante a considerar, es el tiempo que invertimos realizando testeos manuales. Automatizar el testeo y generar un programa que se encargue de probar el sistema es una forma mucho más eficiente de asegurar la calidad del software que estamos construyendo.\n",
    " \n",
    "Desde ahora, todos los programas que ustedes realizarán en sus vidas deben ir de la mano de un programa que lo testea. No olviden que desde ahora, cualquier programa que desarrollen que no esté testeado significa que tiene bugs (*untested code is broken code*).\n",
    "  \n",
    " En este capítulo veremos los conceptos básicos de testing y cómo armar tests unitarios, pero testing es un capítulo que da para un curso completo (de hecho hay un curso de testing). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests unitarios\n",
    "\n",
    "Los tests unitarios se enfocan en testear unidades mínimas de código, donde cada test está encargado de poner a prueba sólo una unidad simple del total de código que conforma el programa. Dentro de las librerías más usadas en Python para elaboración de test unitarios se encuentran: Pytest y Unittest. Ambas nos facilitan la vida a la hora de crear nuestros propias pruebas. Sin pérdida de generalidad, en este curso nos centraremos en **Unittest**.\n",
    "\n",
    "\n",
    "### Unittest \n",
    "\n",
    " La librería `unittest` de Python provee muchas herramientas para crear y correr tests, una de las clases más importantes es `TestCase`. La clases creadas para testear deben heredar de la clase `TestCase`. Por convención todos los métodos que implementamos para testear <b>deben</b> comenzar su nombre con la palabra *test*, de tal forma de que sean reconocidos a la hora de correr el programa de testing en forma automática. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class ChequearNumeros(unittest.TestCase):\n",
    "    # este test debería estar ok\n",
    "    def test_int_float(self):\n",
    "        self.assertEqual(1, 1.0)\n",
    " \n",
    "    # este test debería fallar    \n",
    "    def test_str_float(self):\n",
    "        self.assertEqual(1, \"1\")\n",
    "        \n",
    "# Si quisiéramos ejecutar el test por consola:\n",
    "# if __name__ == \"__main__\":\n",
    "#     unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".F\n",
      "======================================================================\n",
      "FAIL: test_str_float (__main__.ChequearNumeros)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-1-b4f72be58696>\", line 10, in test_str_float\n",
      "    self.assertEqual(1, \"1\")\n",
      "AssertionError: 1 != '1'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.002s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=2 errors=0 failures=1>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pa ra ejecutar los tests aquí en el notebook:\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(ChequearNumeros)\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El punto antes de la *F* indica que el primer test `test_int_float` pasó con éxito, la *F* después del punto indica que el segundo test falló, después aparecen los detalles del test que falló y finalmente el número de tests que se ejecutaron, el tiempo que tomó y el número total de tests que fallaron.\n",
    "\n",
    "Podríamos tener entonces todos los tests que necesitemos dentro de la clase que hereda de unittest.TestCase, siempre y cuando el nombre del método empiece con \"test\". Cada test debe ser completamente independiente de los otros, el resultado del cálculo de un test no debe afectar el resultado de algún otro test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métodos de Aserción \n",
    "\n",
    "En general, en los casos de test asignamos valores a ciertas variables, luego ejecutamos el código que queremos testear y finalmente nos aseguramos de que el resultado coincida con el valor correcto. Los métodos de aserción nos permiten validar los resultados de distintas formas, algunos métodos de aserción (incluidos en la clase TestCase) son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MostrarAsserts(unittest.TestCase):\n",
    "    \n",
    "    def test_aserciones(self):\n",
    "        a = 2\n",
    "        b = a\n",
    "        c = 1. + 1.\n",
    "        self.assertEqual([1,2,3], [1,2,3]) #falla si a != b\n",
    "        self.assertNotEqual(\"hola\", \"chao\") #falla si a == b\n",
    "        self.assertTrue(\"Hola\" == \"Hola\") #falla si bool(x) es False\n",
    "        self.assertFalse(\"Hola\" == \"Chao\") #falla si bool(x) es True\n",
    "        self.assertIs(a, b) #falla si a no es b\n",
    "        self.assertIsNot(a, c) #falla si a es b. Notar que \"is\" implica igualdad (==), pero no al revés, dos objetos distintos\n",
    "                                #pueden tener el mismo valor\n",
    "        self.assertIsNone(None) #falla si x no es None\n",
    "        self.assertIsNotNone(2) #falla si x es None\n",
    "        self.assertIn(2, [2,3,4]) #falla si a no está en b\n",
    "        self.assertNotIn(1, [2,3,4]) #falla si a está en b\n",
    "        self.assertIsInstance(\"Hola\", str) #falla si isinstance(a, b) es False\n",
    "        self.assertNotIsInstance(\"1\", int) #falla si isinstance(a, b) es True\n",
    "    \n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(MostrarAsserts)\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `assertRaises` acepta una excepción, algún *callable* (función o cualquier objeto con el método `__call__` implementado) y un número arbitrario de argumentos (keyworded o no) para ser pasados al método *callable*. La aserción va a invocar al método *callable* con los argumentos y fallará si el método no genera el error esperado. El siguiente código muestra dos formas de cómo usar el método `assertRaises`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.003s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=2 errors=0 failures=0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def average(seq):\n",
    "    return sum(seq) / len(seq)\n",
    "class TestAverage(unittest.TestCase):\n",
    "    def test_python30_zero(self):\n",
    "        self.assertRaises(ZeroDivisionError, average,[])\n",
    "        \n",
    "    def test_python31_zero(self):\n",
    "        with self.assertRaises(ZeroDivisionError):\n",
    "            average([])\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestAverage)\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El administrador de contexto *context manager* nos permite escribir nuestro código de una forma más natural, llamando a la función directamente `average([])` en vez de tener que llamarla en forma indirecta a través de otra función `self.assertRaises(ZeroDivisionError, average,[])`\n",
    "\n",
    "En python 3.4 aparecen nuevos métodos de aserción:\n",
    "``` python\n",
    "assertAlmostEqual(first, second, places=7, msg=None, delta=None)\n",
    "\n",
    "assertNotAlmostEqual(first, second, places=7, msg=None, delta=None)\n",
    "```\n",
    "\n",
    "Testean que `first` y `second` sean aproximadamente (o no aproximadamente) iguales, calculando la diferencia, redondeando al número dado de decimales (el default es 7). Si se provee el argumento `delta` en vez de `places`, la diferencia entre `first` y `second` debe ser menor o igual (o mayor en el caso de `assertNotAlmostEqual`) que `delta`. Si se provee `delta` y `places` se genera un error. Otras aserciones que existen son:\n",
    "\n",
    "\n",
    "``` python\n",
    "assertGreater(first, second, msg=None) # msg es el mensaje que se generará en la aserción\n",
    "\n",
    "assertGreaterEqual(first, second, msg=None)\n",
    "\n",
    "assertLess(first, second, msg=None)\n",
    "\n",
    "assertLessEqual(first, second, msg=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El método setUp\n",
    "Una vez que hemos escrito varios tests, nos damos cuenta que necesitamos armar un grupo de objetos que serán usados como input para comparar los resultados de un test. Además, para algunos tests, lo más probable es que estas variables deban ser modificadas. El método `setUp` nos permite declarar las variables que serán usadas para los tests. Este método se ejecuta antes de cada uno de los tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class ListaEstadisticas(list):\n",
    "    def media(self):\n",
    "        return sum(self) / len(self)\n",
    "    \n",
    "    def mediana(self):\n",
    "        if len(self) % 2:\n",
    "            return self[int(len(self) / 2)]\n",
    "        else:\n",
    "            idx = int(len(self) / 2)\n",
    "            return (self[idx] + self[idx-1]) / 2\n",
    "        \n",
    "    def moda(self):\n",
    "        freqs = defaultdict(int)\n",
    "        for item in self:\n",
    "            freqs[item] += 1\n",
    "        moda_freq = max(freqs.values())\n",
    "        modas = []\n",
    "        for item, value in freqs.items():\n",
    "            if value == moda_freq:\n",
    "                modas.append(item)\n",
    "        return modas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 3, 3, 4]\n",
      "[1, 2, 2, 3, 3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.016s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestearEstadisticas(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        self.stats = ListaEstadisticas([1,2,2,3,3,4])\n",
    "        \n",
    "    def test_media(self):\n",
    "        print(self.stats)\n",
    "        self.assertEqual(self.stats.media(), 2.5)\n",
    "        \n",
    "    def test_mediana(self):\n",
    "        self.assertEqual(self.stats.mediana(), 2.5)\n",
    "        self.stats.append(4)\n",
    "        self.assertEqual(self.stats.mediana(), 3)\n",
    "        \n",
    "    def test_moda(self):\n",
    "        print(self.stats)\n",
    "        self.assertEqual(self.stats.moda(), [2,3])\n",
    "        self.stats.remove(2)\n",
    "        self.assertEqual(self.stats.moda(), [3])\n",
    "                \n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestearEstadisticas)\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `setUp` nunca es llamado explícitamente adentro de alguno de los tests, unittest lo hace por nosotros.  Además podemos ver que en `test_mediana` se modificó la lista agregando un 4, pero luego en `test_moda` la lista es la misma que al principio. Esto ocurre porque `setUp` se encarga de volver a inicializar las variables necesarias al comienzo de cada test. Esto nos ayuda bastante a no repetir código innecesariamente. \n",
    "\n",
    "Además del método `setUp`, `TestCase` nos ofrece el método `tearDown`, que puede ser usado para \"limpiar\" después de que se terminó de ejecutar el tests. Por ejemplo, si el tests tiene la necesidad de crear algunos archivos, estos archivos deberían eliminarse del sistema para no generar problemas con la ejecución del programa, de tal forma de asegurar que el sistema está en el mismo estado en que estaba antes de ejecutar los tests. Ejemplo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "escribiendo archivo temporal...\n",
      "['Hola']\n",
      "Eliminando archivos temporales...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FAIL: test_str (__main__.TestearArchivo)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-8-2343ec3926a5>\", line 21, in test_str\n",
      "    self.assertEqual(self.diccionario[1], d)\n",
      "AssertionError: 'Hola' != ['Hola']\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=1>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "class TestearArchivo(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        self.archivo = open(\"prueba.txt\",'w')\n",
    "        self.diccionario = {1 : \"Hola\", 2 : \"Chao\" }\n",
    "\n",
    "    def tearDown(self):\n",
    "        self.archivo.close()\n",
    "        print(\"Eliminando archivos temporales...\")\n",
    "        os.remove(\"prueba.txt\")\n",
    "        \n",
    "    def test_str(self):\n",
    "        print(\"escribiendo archivo temporal...\")\n",
    "        self.archivo.write(self.diccionario[1])\n",
    "        self.archivo.close()\n",
    "        self.archivo = open(\"prueba.txt\",'r')\n",
    "        d = self.archivo.readlines()\n",
    "        print(d)\n",
    "        self.assertEqual(self.diccionario[1], d)\n",
    "                        \n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestearArchivo)\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organizar los tests\n",
    "\n",
    "Cuando testeamos un programa, rápidamente nos empezamos a llenar de código sólo de testeo. Para solucionar este problema podemos organizar nuestros módulos que contienen tests (objetos TestCase) en módulos más generales llamados test suites (objetos TestSuite), que contienen colecciones de tests. Ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.003s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=2 errors=0 failures=0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestAritmetico(unittest.TestCase):\n",
    "\n",
    "    def test_arit(self):\n",
    "        self.assertEqual(1+1,2)\n",
    "\n",
    "class TestAritmetico2(unittest.TestCase):\n",
    "\n",
    "    def test_arit2(self):\n",
    "        self.assertNotEqual(2*1,1)\n",
    "        \n",
    "Tsuite = unittest.TestSuite()\n",
    "Tsuite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestAritmetico))\n",
    "Tsuite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestAritmetico2))\n",
    "unittest.TextTestRunner().run(Tsuite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cómo ignorar tests fallidos\n",
    "\n",
    "Muchas veces sabemos que algunos de los tests van a fallar en nuestro programa. Por ejemplo, si tenemos una función que no está terminada, o si estamos corriendo una versión distinta de alguna plataforma, sabemos que el test fallará. En estos casos podríamos querer que no se reporte la falla o que el test no se ejecute. Afortunadamente, Python nos provee de algunos elementos para marcar tests que sabemos que van a fallar o para que los ignore bajo ciertas condiciones. Estos elementos son: `expectedFailure`, `skip(reason)`, `skipIf(condition, reason)`, `skipUnless(condition, reason)`. Ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sssFx\n",
      "======================================================================\n",
      "FAIL: test_que_no_sabiamos_que_falla (__main__.IgnorarTests)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-95c055dd7b86>\", line 8, in test_que_no_sabiamos_que_falla\n",
      "    self.assertEqual(False, True)\n",
      "AssertionError: False != True\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.010s\n",
      "\n",
      "FAILED (failures=1, skipped=3, expected failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=5 errors=0 failures=1>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "import sys\n",
    "\n",
    "class IgnorarTests(unittest.TestCase):\n",
    "    \n",
    "    # Este test fallará\n",
    "    def test_que_no_sabiamos_que_falla(self):\n",
    "        self.assertEqual(False, True)\n",
    "    \n",
    "    @unittest.expectedFailure\n",
    "    def test_sabemos_que_falla(self):\n",
    "        self.assertEqual(False, True)\n",
    "        \n",
    "    @unittest.skip(\"Test inútil\")\n",
    "    def test_ignorar(self):\n",
    "        self.assertEqual(False, True)\n",
    "        \n",
    "    @unittest.skipIf(sys.version_info.minor == 5, \"no funciona en 3.1\")\n",
    "    def test_ignorar_if(self):\n",
    "        self.assertEqual(False, True)\n",
    "        \n",
    "    @unittest.skipUnless(sys.platform.startswith('Linux'), \"no funciona en linux\")\n",
    "    def test_ignorar_unless(self):\n",
    "        self.assertEqual(False, True)\n",
    "    \n",
    "\n",
    "                        \n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(IgnorarTests)\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La \"x\" en la primera línea significa \"falla esperada\" (expected failure), la \"s\" significa \"test ignorado\" (skipped test), la \"F\" significa una falla real (real failure). Los resultados están en orden alfabético."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
